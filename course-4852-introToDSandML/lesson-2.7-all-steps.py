# -*- coding: utf-8 -*-
"""course-4852-introToDSandML-2.7.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dqJVFtCvifG8MG07ejiyeHZ72E26qgEa
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import requests, zipfile, io
# %matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import tree
from sklearn.model_selection import cross_val_score

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import precision_score
from sklearn.model_selection import train_test_split

dataset_url = 'https://stepik.org/media/attachments/course/4852/train_data_tree.csv'
df = pd.read_csv(dataset_url)

df.head()

X = df.drop(['num'],axis = 1)
y = df.num

X = pd.get_dummies(X)

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3, random_state = 42, shuffle = True)

X_train.head()

# Step 3 - find information gain
clf = DecisionTreeClassifier(criterion='entropy', random_state = 42)

clf.fit(X_train, y_train)
predictions = clf.predict(X_test)

from IPython.display import SVG
from graphviz import Source
from IPython.display import display

#tree.plot_tree(clf, filled=True)

print(tree.plot_tree(clf.fit(X,y)))
graph = Source(tree.export_graphviz(clf, out_file=None, feature_names=list(X), class_names=['Negative', 'Positive'], filled=True))
display(SVG(graph.pipe(format='svg')))

left_node = clf.tree_.children_left[0]    # 0 - index of the head of the tree
num_samples_left = clf.tree_.n_node_samples[left_node]  # num_samples_left
entropy_left = clf.tree_.impurity[left_node]
right_node = clf.tree_.children_right[0]
num_samples_right = clf.tree_.n_node_samples[right_node]
entropy_right = clf.tree_.impurity[right_node]
print(f"{clf.tree_.impurity[0] - (num_samples_left/len(df)*entropy_left + num_samples_right/len(df)*entropy_right)}")

# Step 6

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_iris


iris = load_iris()
X = pd.DataFrame(iris.data)
y = pd.DataFrame(iris.target)

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25, random_state = 42, shuffle = True)

X_train.head()

dt = DecisionTreeClassifier()

dt.fit(X_train, y_train)

predicted = dt.predict(X_test)

from sklearn.metrics import accuracy_score
print(accuracy_score(y_test, predicted))
print(precision_score(y_test, predicted, average='weighted'))

# step 10
from sklearn.model_selection import GridSearchCV

parameters = {'max_depth': range(1,11), 'min_samples_split': range(2,11),'min_samples_leaf':range(1,11)}

dt = DecisionTreeClassifier()

search = GridSearchCV(dt, parameters, cv=5)
search.fit(iris.data, iris.target)

best_tree = search.best_estimator_

print(search.best_estimator_)

# step 11
from sklearn.model_selection import RandomizedSearchCV

parameters = {'max_depth': range(1,11), 'min_samples_split': range(2,11),'min_samples_leaf':range(1,11)}

dt = DecisionTreeClassifier()

search = RandomizedSearchCV(dt, parameters, cv=5)
search.fit(iris.data, iris.target)

best_tree = search.best_estimator_

# step 12
# train, test datasets already defined

from sklearn.model_selection import GridSearchCV

X_train = train.drop(['y'], axis='columns')
y_train = train.y

parameters = {'max_depth': range(1,11), 'min_samples_split': range(2,11),'min_samples_leaf':range(1,11)}

dt = DecisionTreeClassifier()

search = GridSearchCV(dt, parameters, cv=5)
search.fit(X_train, y_train)

best_tree = search.best_estimator_

predictions = best_tree.predict(test)

# step 13
# y, predicitons - already defined
import pandas as pd
from sklearn.metrics import confusion_matrix


conf_matrix = confusion_matrix(y, predictions)

