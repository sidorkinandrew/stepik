# -*- coding: utf-8 -*-
"""course-4852-introToDSandML_2.9.ipynb

Automatically generated by Colaboratory.

https://colab.research.google.com/drive/13Tnqm4FcQbSZKubghvpeETWxGrku3lM_#scrollTo=pw3tTqPNI9zt


"""

# 2.9.Stepik ML contest
## step 2

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np

# %matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns

# reading datasets
events_data = pd.read_csv('https://stepik.org/media/attachments/course/4852/event_data_train.zip', compression='zip')
submissions_data = pd.read_csv('https://stepik.org/media/attachments/course/4852/submissions_data_train.zip', compression='zip')

# adding new columns
events_data['date'] = pd.to_datetime(events_data.timestamp, unit='s')
events_data['day'] = events_data.date.dt.date

submissions_data['date'] = pd.to_datetime(submissions_data.timestamp, unit='s')
submissions_data['day'] = submissions_data.date.dt.date

# calculating for each user count correct submimit
users_scores = submissions_data.pivot_table(index='user_id',
                        columns='submission_status',
                        values='step_id',
                        aggfunc='count',
                        fill_value=0).reset_index()

users_data = events_data.groupby('user_id', as_index=False) \
    .agg({'timestamp': 'max'}).rename(columns={
    'timestamp': 'last_timestamp'
})

now = 1526772811
drop_out_treshold = 30 * 24 * 60 * 60 # пороговое значение
users_data['is_gone_user'] = (now - users_data.last_timestamp) > drop_out_treshold

users_data = users_data.merge(users_scores, on='user_id', how='outer')
users_data = users_data.fillna(0)

users_events_data = events_data.pivot_table(index='user_id',
                        columns='action',
                        values='step_id', 
                        aggfunc='count', 
                        fill_value=0).reset_index()
users_data = users_data.merge(users_events_data, how='outer')

users_days = events_data.groupby('user_id').day.nunique()
users_days = events_data.groupby('user_id').day.nunique().to_frame().reset_index()
users_data = users_data.merge(users_days, how='outer')

users_data['passed_course'] = users_data.passed > 175

users_data.head()

submissions_data.head()

users_data.user_id.nunique()

events_data.user_id.nunique()

users_data[users_data.passed_course].day.median()

users_data[users_data.passed_course].day.hist()

## step 4

user_min_time = events_data.groupby('user_id', as_index=False) \
    .agg({'timestamp': 'min'}) \
    .rename({'timestamp': 'min_timestamp'}, axis=1)

user_min_time.head()

users_data = users_data.merge(user_min_time, how='outer')

users_data.head()

event_data_train = pd.DataFrame() # сюда сложим уже отобранные данные

for user_id in users_data.user_id:  # WRONG WAY
    min_user_time = users_data[users_data.user_id == user_id].min_timestamp[0]
    break

min_user_time

user_id

users_data[['user_id', 'min_timestamp']].head(3)

user_indexes = {_user_id: users_data[users_data.user_id == _user_id].index for _user_id in users_data.user_id}

user_indexes

min_learn_time_user = {_user_id: users_data.iloc[user_indexes[_user_id]].min_timestamp  + 3 * 24 * 60 * 60 for _user_id in users_data.user_id}

#events_data['min_time_threshold'] = events_data.apply(lambda x: min_learn_time_user[x['user_id']], axis=1)   #min_timestamps_user[x['user_id']])

#events_user_learning_time = events_data.drop(events_data[events_data['timestamp']>events_data['min_time_threshold']].index)

#events_user_learning_time

user_events_data

user_events_data = {}

for user_id in users_data.user_id:
    min_user_time = users_data[users_data.user_id == user_id].min_timestamp.item()
#    time_treshold = min_user_time + 3 * 24 * 60 * 60 # minimal timestamp + 3 days in seconds
#    user_events_data[user_id] = events_data[(events_data.user_id == user_id) & (events_data.timestamp < time_treshold)]

# TOO SLOW !!!

#event_data_train = event_data[]

## step 5

events_data['user_time'] = events_data.user_id.map(str) + '_' + events_data.timestamp.map(str)

events_data.head(5555)

learning_time_treshold = 3 * 24 * 60 * 60
learning_time_treshold

user_learning_time_treshold = user_min_time.user_id.map(str) + '_' + (user_min_time.min_timestamp + learning_time_treshold).map(str)

user_learning_time_treshold.head()

## step 6

user_min_time['user_learning_time_treshold'] = user_learning_time_treshold

events_data = events_data.merge(user_min_time[['user_id', 'user_learning_time_treshold']], how='outer')

events_data.shape

events_data.head()

events_data_train = events_data[events_data.user_time <= events_data.user_learning_time_treshold]

events_data_train.head()

events_data_train.shape

submissions_data.head()

submissions_data[submissions_data['submission_status']=='wrong'].groupby(['step_id']).count().submission_status.idxmax()

pd.crosstab(submissions_data['step_id'], submissions_data['submission_status']).sort_values('wrong', ascending=False)

## step 8

## step 9

## step 10

events_data_train.groupby('user_id').day.nunique().max()

## step 11

submissions_data['user_time'] = submissions_data.user_id.map(str) + '_' + submissions_data.timestamp.map(str)
submissions_data = submissions_data.merge(user_min_time[['user_id', 'user_learning_time_treshold']], how='outer')
submissions_data_train = submissions_data[submissions_data.user_time <= submissions_data.user_learning_time_treshold]
submissions_data_train.groupby('user_id').day.nunique().max()

X = submissions_data_train.groupby('user_id').day.nunique(). \
    to_frame().reset_index().rename(columns=({
        'day': 'days'
}))

X.head()

steps_tried = submissions_data_train.groupby('user_id').step_id \
    .nunique().to_frame().reset_index().rename(columns={
        'step_id': 'steps_tried'
})

steps_tried.head()

X = X.merge(steps_tried, on='user_id', how='outer')

X.shape

X.head()

submissions_data_train.pivot_table(index='user_id',
                        columns='submission_status',
                        values='step_id', 
                        aggfunc='count', 
                        fill_value=0).reset_index().head()

X = X.merge(submissions_data_train.pivot_table(index='user_id',
                        columns='submission_status',
                        values='step_id', 
                        aggfunc='count', 
                        fill_value=0).reset_index())

X.head()

X['correct_ratio'] = X.correct / (X.correct + X.wrong)

X.head()

## step 12

X = X.merge(events_data_train.pivot_table(index='user_id',
                        columns='action',
                        values='step_id', 
                        aggfunc='count', 
                        fill_value=0).reset_index()[['user_id', 'viewed']], how='outer')

X.head()

X.shape

X

X = X.fillna(0)

X = X.merge(users_data[['user_id', 'passed_course', 'is_gone_user']], how='outer')

X.head()

X[(X.is_gone_user == False) & (X.passed_course == False)].head()

X[~((X.is_gone_user == False) & (X.passed_course == False))].head()

X = X[~((X.is_gone_user == False) & (X.passed_course == False))]

X.head()

X.shape

X.groupby(['passed_course', 'is_gone_user']).user_id.count()

y = X.passed_course.map(int)

X = X.drop(['passed_course', 'is_gone_user'], axis=1)

X = X.set_index(X.user_id)
X = X.drop('user_id', axis=1)

X.head()

## step 13

pd.DataFrame(X).to_csv('./drive/My Drive/stepic/4852-introToDSandML/X_data.csv')
pd.DataFrame(y).to_csv('./drive/My Drive/stepic/4852-introToDSandML/Y_data.csv')

from sklearn.linear_model import LogisticRegressionCV
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import f1_score, plot_confusion_matrix

clf = LogisticRegressionCV(cv=5)

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=41)

clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)

np.set_printoptions(precision=4)

print(clf.score(X_test,y_test))
print(clf.coef_)
print(clf.intercept_)
print(cross_val_score(clf,X_train,y_train,cv=5).mean())
print(f1_score(y_test, y_pred))
print(plot_confusion_matrix(clf, X_test, y_test))

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree
from sklearn.model_selection import train_test_split, GridSearchCV,cross_val_score

from IPython.display import SVG
from graphviz import Source
from IPython.display import display


X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)

dt = DecisionTreeClassifier(criterion='entropy')
parameters = {'max_depth': range(3,6), 'max_leaf_nodes':range(6,15), 'min_samples_leaf': range(1,4),'min_samples_split':range(2,5)}
grid_search_cv_clf = GridSearchCV(dt,parameters,cv=4)
grid_search_cv_clf.fit(X_train,y_train)

model = grid_search_cv_clf.best_estimator_
print(grid_search_cv_clf.best_params_, cross_val_score(model,X_train,y_train,cv=4).mean())

graph = Source(tree.export_graphviz(model, out_file=None,
                                    class_names=['Negative', 'Positive'], filled=True, rounded=True,
                                    special_characters=True))
display(SVG(graph.pipe(format='svg')))

y_pred = model.predict(X_test)

from sklearn.metrics import auc
from sklearn.metrics import plot_roc_curve
from sklearn.metrics import roc_auc_score
print(roc_auc_score(y_test, y_pred))

print(f1_score(y_test, y_pred))

print(plot_confusion_matrix(model, X_test, y_test))